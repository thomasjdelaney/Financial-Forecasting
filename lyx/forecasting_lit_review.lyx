#LyX file created by tex2lyx 2.3
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin C:/Financial_Forecasting/lyx/
\textclass article
\begin_preamble
%\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{times}
%\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{subcaption}



\fancyhf{}
\lhead{Thomas J. Delaney}
\rhead{Forecasting Methods}
\cfoot{\thepage}

\newtheorem{theorem}{Theorem}\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\boldnabla}{\mbox{\boldmath$\nabla$}} % to be used in mathmode
\newcommand{\cbar}{\overline{\mathbb{C}}}% to be used in mathmode
\newcommand{\diff}[2]{\frac{d #1}{d #2}}% to be used in mathmode
\newcommand{\difff}[2]{\frac{d^2 #1}{d #2^2}}% to be used in mathmode
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}} % to be used in mathmode
\newcommand{\pdifff}[2]{\frac{\partial^2 #1}{\partial #2^2}}% to be used in mathmode
\newcommand{\upperth}{$^{\mbox{\footnotesize{th}}}$}%to be used in text mode
\newcommand{\vect}[1]{\mathbf{#1}}% to be used in mathmode
\newcommand{\curl}[1]{\boldnabla \times \vect{#1}} % to be used in mathmode
\newcommand{\divr}[1]{\boldnabla \cdot \vect{#1}} %to be used in mathmode
\newcommand{\modu}[1]{\left| #1 \right|} %to be used in mathmode
\newcommand{\brak}[1]{\left( #1 \right)} % to be used in mathmode
\newcommand{\comm}[2]{\left[ #1 , #2 \right]} %to be used in mathmode
\newcommand{\dop}{\vect{d}} %to be used in mathmode
\newcommand{\cov}{\text{cov}} %to be used in mathmode
\newcommand{\var}{\text{var}} %to be used in mathmode
\newcommand{\mb}{\mathbf} %to be used in mathmode
\newcommand{\bs}{\boldsymbol} %to be used in mathmode
% Title Page
\title{Forecasting Methods: An Overview}

\author{Thomas J. Delaney}


\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks 0
\pdf_bookmarksnumbered 0
\pdf_bookmarksopen 0
\pdf_bookmarksopenlevel 1
\pdf_breaklinks 0
\pdf_pdfborder 0
\pdf_colorlinks 0
\pdf_backref section
\pdf_pdfusetitle 0
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 2
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
An overview of forecasting methods, from the simplest to the most up to date.
\end_layout

\begin_layout Subsection
What is Forecasting?
\end_layout

\begin_layout Standard
A 
\shape italic
time series
\shape default
 is list of measurements or recordings ordered by the time at which those measurements were taken, or those recordings were made. 
\shape italic
Forecasting
\shape default
 is the act of predicting the future value of a quantity (usually a time series) with some measure of confidence. Every forecasting method consists of constructing a model of the series, and basing a mean prediction and prediction interval on that model.
\end_layout

\begin_layout Subsubsection
Example: The Na誰ve Method
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:naive_method"

\end_inset

 For example, one of the simplest forecasting methods is the 
\shape italic
na誰ve
\shape default
 method. This consists of forecasting that the next value of the time series will be sampled from a normal distribution (aka. Gaussian distribution) with mean equal to the latest value of the time series, and standard deviation equal to the standard deviation of the one step changes in the time series. Using this model, the forecasted value for the time series is same as the latest value, and the prediction intervals are calculated using the z-distribution.
\end_layout

\begin_layout Standard
More formally, if 
\begin_inset Formula $y_1, \dots, y_T \in \mathbb{R}$
\end_inset

 is a time series and 
\begin_inset Formula $d_1, \dots, d_{T-1}$
\end_inset

 are the one step differences of that series, then according to the na誰ve model 
\begin_inset Formula \begin{equation}\label{eq:naive_method}
	\hat{y}_{T+1 | 1, \dots, T} \sim \mathcal{N}(y_T, \sigma^2)
\end{equation}
\end_inset

where 
\begin_inset Formula $\sigma^2$
\end_inset

 is the variance of 
\begin_inset Formula $d_1, \dots, d_{T-1}$
\end_inset

. As for prediction intervals, since the z-statistic for 
\begin_inset Formula $95\%$
\end_inset

 is 
\begin_inset Formula $1.96$
\end_inset

, the 
\begin_inset Formula $95\%$
\end_inset

 prediction interval for 
\begin_inset Formula $\hat{y}_{T+1 | 1, \dots, T}$
\end_inset

 is 
\begin_inset Formula $\pm1.96\sigma$
\end_inset

.
\end_layout

\begin_layout Section
Residuals, Normality& Prediction Intervals
\end_layout

\begin_layout Standard
In a forecasting context, a residual is the difference between a given time series value and the one step forecast for that value, 
\begin_inset Formula $e_t = y_t - \hat{y}_{t | 1, \dots , t-1}$
\end_inset

. Given some forecasting model,these residuals are usually used to determine the amount of noise in the model and calculate the prediction intervals. It should be noted that since the z-distribution is always used to calculate the prediction intervals, the residuals of the model must be distributed normally. Otherwise the intervals will be inaccurate. Effectively, the noise in any model is always modelled as white noise, 
\begin_inset Formula \begin{equation}\label{eq:white_noise}
	\epsilon_t \sim \mathcal{N}(0, \sigma^2)
\end{equation}
\end_inset

where 
\begin_inset Formula $\sigma^2$
\end_inset

 is the variance of the residuals.
\end_layout

\begin_layout Standard
In the example given in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:naive_method"
plural "false"
caps "false"
noprefix "false"

\end_inset

, since the model forecasts the next value in the series to be the same as the last, the residuals are the one step differences of the series. Another way of formulating the model given by equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:naive_method"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1 | 1, \dots, T} = y_T + \epsilon_t
\end{align}
\end_inset


\end_layout

\begin_layout Section
Random Walks, Transformations, & Stationarity
\end_layout

\begin_layout Subsection
Random Walk Forecasting
\end_layout

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% consider putting random walk in with ETS and ARIMA altogether
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Random walk is another simple method for forecasting in which the one step change in the time series is modelled. The simplest version models the one step change as a normal distribution with zero mean. This is the same as the na誰ve model in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:naive_method"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The model can be expanded to include a 
\shape italic
drift
\shape default
 term so that 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1|1, \dots, T} - y_T \sim \mathcal{N}(c, \sigma^2)
\end{align}
\end_inset

where 
\begin_inset Formula $c \in \mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\sigma^2$
\end_inset

 is the variance of the residuals. If 
\begin_inset Formula $c > 0$
\end_inset

 then the series will show an upward linear trend, and if 
\begin_inset Formula $c<0$
\end_inset

 the series will show a downward linear trend. Another way of formulating this model is 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1|1, \dots, T} = y_T + c + \epsilon_t
\end{align}
\end_inset

where 
\begin_inset Formula $\epsilon_t$
\end_inset

 is defined by equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:white_noise"
plural "false"
caps "false"
noprefix "false"

\end_inset

, as usual.
\end_layout

\begin_layout Subsection
Seasonally Adjusted Random Walk
\end_layout

\begin_layout Standard
A random walk model can be used to forecast time series with a seasonal pattern. If there are 
\begin_inset Formula $m$
\end_inset

 seasons, that is, if the seasonality has a period of 
\begin_inset Formula $m$
\end_inset

 time-steps then the forecasted value will be based on the value of the previous season, rather than the previous time step 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1|1, \dots, T}  - y_{T+1-m} \sim \mathcal{N}(c,\sigma^2) \hspace{5mm} \text{or} \hspace{5mm} \hat{y}_{T+1|1, \dots, T} =  y_{T+1-m} + c + \epsilon_t
\end{align}
\end_inset


\end_layout

\begin_layout Standard
These random walk models are sometimes called 
\shape italic
difference models
\shape default
 because the forecast is based upon the difference between previous values of the series, with some lag applied.
\end_layout

\begin_layout Subsection
Transformations for Stationarity
\end_layout

\begin_layout Standard
A stationary time series is one whose properties do not depend on the time at which the series is observed, or more precisely if 
\begin_inset Formula $\{y_t\}$
\end_inset

 is a stationary time series, then for all 
\begin_inset Formula $s$
\end_inset

, the distribution of 
\begin_inset Formula $(y_t, \dots ,y_{t+s})$
\end_inset

 does not depend on 
\begin_inset Formula $t$
\end_inset

. A white noise time series (equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:white_noise"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is an example of a stationary series. The aim of most simple forecasting models is to capture all the variance of a time series, except for some additional or multiplicative white noise. This means that the model will often consist of applying transformations to the time series until the series is stationary.
\end_layout

\begin_layout Standard
For example, the na誰ve model captures the amplitude or 
\shape italic
level
\shape default
 of the time series by taking the one step difference, leaving only white noise thereafter. A seasonal random walk model with a non-zero drift parameter will capture the amplitude of the time series at each season, and the overall trend of the time series, leaving only white noise thereafter. Some non-linear trends can be captured using a Box-Cox transformation. 
\begin_inset Formula \begin{align}
	y_{t, (\lambda)} = \begin{cases}
					\frac{y_t^\lambda - 1}{\lambda} &\quad \text{if } \lambda \neq 0 \\
					\ln(y_t) &\quad \text{if } \lambda = 0 
				\end{cases}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
If multiplicative noise is observed in the time series, it's common to apply a log transformation in order to the make the noise additive, or a Box-Cox transformation with 
\begin_inset Formula $\lambda = 0$
\end_inset

. In the case where the time series may take values less than or equal to zero, the Yeo-Johnson transforamtion is used to capture non-linearity, 
\begin_inset Formula \begin{align}
	y_{t, (\lambda)} = \begin{cases}
					\frac{(y_t + 1)^\lambda - 1}{\lambda} &\quad \text{if } \lambda \neq 0, y_t \geq 0 \\
					\ln(y_t + 1) &\quad \text{if } \lambda = 0, y_t \geq 0 \\
					-\frac{[(-y_t + 1)^{2-\lambda}-1]}{2-\lambda} &\quad \text{if } \lambda \neq 2, y_t < 0 \\
					-\ln(-y_t + 1) &\quad \text{if } \lambda = 2, y_t < 0
				\end{cases}
\end{align}
\end_inset

where 
\begin_inset Formula $0 \leq \lambda \leq 2$
\end_inset

.
\end_layout

\begin_layout Section
ARIMA
\end_layout

\begin_layout Subsection
Auto-regression models
\end_layout

\begin_layout Standard
Auto-regression models are exactly what they sound like. They use a linear combination of some of the previous elements of the time series to forecast the upcoming values. The number of values used is determined by the 
\shape italic
degree
\shape default
, 
\begin_inset Formula $p$
\end_inset

. A degree-
\begin_inset Formula $p$
\end_inset

 auto-regressive model can be written as 
\begin_inset Formula \begin{equation}\label{eq:ar_model}
	\hat{y}_{T|1, \dots, T-1} = c + \phi_1 y_{T-1} + \phi_2 y_{T-2} + \cdots + \phi_p y_{T-p} + \epsilon_t
\end{equation}
\end_inset

where 
\begin_inset Formula $\epsilon_t$
\end_inset

 is the usual white noise. A model of this kind of often denoted as 
\begin_inset Formula $AR(p)$
\end_inset

.
\end_layout

\begin_layout Standard
Notice that the 
\begin_inset Formula $c$
\end_inset

 term can capture a linear trend in the series. An 
\begin_inset Formula $AR(1)$
\end_inset

 model with 
\begin_inset Formula $\phi_1 = 1$
\end_inset

 is equivalent to a random walk with a trend.
\end_layout

\begin_layout Standard
In order for the modelled series to be stationary some restrictions must be applied to the parameter values. Specifically, for an 
\begin_inset Formula $AR(p)$
\end_inset

 model the roots of the polynomial 
\begin_inset Formula \begin{align}
	z^p - \sum_{i=1}^{p} \phi_{i}z^{p-i}
\end{align}
\end_inset

must lie within the complex unit circle, 
\begin_inset Formula $\modu{z_i} < 1$
\end_inset

. Practically, this amounts to having 
\begin_inset Formula $\modu{\phi_1}<1$
\end_inset

 for an 
\begin_inset Formula $AR(1)$
\end_inset

 model, and 
\begin_inset Formula $\modu{\phi_2}<1$
\end_inset

, and 
\begin_inset Formula $\phi_1 + \phi_2 <1$
\end_inset

 for 
\begin_inset Formula $AR(2)$
\end_inset

.
\end_layout

\begin_layout Standard
The most suitable order for an 
\begin_inset Formula $AR(p)$
\end_inset

 model can be determined by looking at the 
\begin_inset Formula $ACF$
\end_inset

 and 
\begin_inset Formula $PACF$
\end_inset

 of the time series. The lag beyond which the 
\begin_inset Formula $PACF$
\end_inset

 appears to be zero is the most suitable order.
\end_layout

\begin_layout Subsection
Moving-average models
\end_layout

\begin_layout Standard

\shape italic
Moving average model
\shape default
 is a poor name for this model, but it is the conventional name. A better name would be 
\shape italic
residual-regression model
\shape default
 because the forecast is a linear combination of past residuals. A moving average model or order q can be written as 
\begin_inset Formula \begin{equation}
	\hat{y}_{T|1, \dots, T-1} = c + \epsilon_t + \theta_1 \epsilon_{t-1} + \cdots + \theta_q \epsilon_{t-q}
\end{equation}
\end_inset

where 
\begin_inset Formula $\epsilon_t$
\end_inset

 is the usual white noise. This type of model is usually denoted 
\begin_inset Formula $MA(q)$
\end_inset

.
\end_layout

\begin_layout Standard
If a moving average model can be expressed as an infinite degree AR model, the moving average model is called 
\shape italic
invertible
\shape default
.
\end_layout

\begin_layout Subsection
ARIMA models
\end_layout

\begin_layout Standard
Auto-regression, difference, and moving average models can be combined. A combination of an 
\begin_inset Formula $AR(p)$
\end_inset

, 
\begin_inset Formula $d$
\end_inset

-step difference, and 
\begin_inset Formula $MA(q)$
\end_inset

 model is denoted 
\begin_inset Formula $ARIMA(p,d,q)$
\end_inset

. 
\begin_inset Formula $ARIMA$
\end_inset

 stands for 
\shape italic
Auto-regressive integrated moving average
\shape default
. To construct an 
\begin_inset Formula $ARIMA(p,d,q)$
\end_inset

 model, first 
\begin_inset Formula $\{y_t\}$
\end_inset

 is transformed to 
\begin_inset Formula $\{ y_t^{\prime} \}$
\end_inset

 by taking the 
\begin_inset Formula $d$
\end_inset

-step difference, then 
\begin_inset Formula \begin{equation}
	\hat{y}_{T|1,\dots,T-1}^{\prime} = c + \phi_1 y_{T-1}^{\prime} + \cdots + \phi_{p} y_{T-p}^{\prime} + \epsilon_t + \theta_{1} \epsilon_{t-1} + \cdots + \theta_{q} \epsilon_{t-q}
\end{equation}
\end_inset

The degree of the 
\begin_inset Formula $AR$
\end_inset

 and the order of the 
\begin_inset Formula $MA$
\end_inset

 that should be used can be determined from a combination of the 
\begin_inset Formula $ACF$
\end_inset

 and the 
\begin_inset Formula $PACF$
\end_inset

. The number of differences to use is generally two or fewer. This can be determined by examining the long term trend of the data.
\end_layout

\begin_layout Subsubsection
Seasonal ARIMA models
\end_layout

\begin_layout Standard
In order to capture seasonal behaviour in the time series, a 
\shape italic
seasonal ARIMA
\shape default
 model can be used. This is an additional 
\begin_inset Formula $ARIMA$
\end_inset

 model where all the lags are the length of one season. The length of a season will have to be found by inspection. The additional seasonal 
\begin_inset Formula $ARIMA$
\end_inset

 terms are just multiplied by the standard 
\begin_inset Formula $ARIMA$
\end_inset

 terms.
\end_layout

\begin_layout Section
Error, Trend, Smoothing models (ETS models)
\end_layout

\begin_layout Subsection
Simple Exponential Smoothing
\end_layout

\begin_layout Standard
If a time series has no clear trend, seasonality, or cycle but appears to vary around some fixed amplitude, simple exponential smoothing might be a suitable forecasting method. In simple exponential smoothing, the forecast is an weighted sum of the series's previous values where the weights decrease exponentially going backwards in time. 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1 | T,\dots,1} = \alpha y_T + \alpha(1-\alpha)y_{T-1} + \alpha(1-\alpha)^2y_{T-2} + \cdots
\end{align}
\end_inset

where 
\begin_inset Formula $0 \leq \alpha \leq 1$
\end_inset

 is the smoothing parameter. If 
\begin_inset Formula $\alpha$
\end_inset

 is close to 
\begin_inset Formula $0$
\end_inset

, a large weight will be given to values in the distant past. If 
\begin_inset Formula $\alpha$
\end_inset

 is close to 
\begin_inset Formula $1$
\end_inset

, more recent values will get a larger weight, which usually makes sense. In the extreme case where 
\begin_inset Formula $\alpha = 1$
\end_inset

 the forecast will be same as the mean forecast of the na誰ve method.
\end_layout

\begin_layout Standard
Basically just decompose the time series into level, trend, and seasonal components. The seasonal component can be multiplicative or additive. Simple exponential smooting is usually written in one of two ways. The weighted average form is 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1|T,\dots,1} = \alpha y_T + \left(  1 - \alpha \right)\hat{y}_{T|T-1,\dots,1}
\end{align}
\end_inset

Note that the second term on the right hand side is a forecasted value, and therefore accounts for all the values of 
\begin_inset Formula $\left\{ y_t \right\}$
\end_inset

 where 
\begin_inset Formula $t < T$
\end_inset

.
\end_layout

\begin_layout Standard
The component form is 
\begin_inset Formula \begin{align}
	\hat{y}_{T+1|T,\dots,1} &= \ell_T \label{eq:ses_forecast}\\
	\ell_T &= \alpha y_T	+ (1 - \alpha)\ell_{T-1} \label{eq:ses_smoothing}
\end{align}
\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ses_forecast"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the 
\shape italic
forecast equation
\shape default
, and equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ses_smoothing"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the 
\shape italic
smoothing equation
\shape default
 or the 
\shape italic
level equation
\shape default
. The practicality of the component form will be clearer when trend and seasonal components are introduced.
\end_layout

\begin_layout Standard
When implementing exponential smoothing, the 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\ell_0$
\end_inset

 are parameters that must be fitted. The usual way of fitting the parameters is by minimising the sum of the squared errors between the timer series and the fitted values, 
\begin_inset Formula \begin{align}
	SSE =  \sum_{t=1}^T\left( y_t - \hat{y}_{t|t-1} \right)^2
\end{align}
\end_inset

This is a non-linear optimisation problem for which there is no analytical solution, so an optimisation algorithm must be used. Usually this will be built into whatever programming language is used for implementation.
\end_layout

\begin_layout Subsection
Holt's Linear Trend Method
\end_layout

\begin_layout Standard
Simple exponential smooting can be extended to time series with a trend by including another smoothing equation. 
\begin_inset Formula \begin{align}
	\hat{y}_{T+h|T,\dots,1} &= \ell_T + hb_T \label{eq:holt_forecast}\\
	\ell_T &= \alpha y_T + (1 - \alpha)\left(\ell_{T-1} + b_{T-1}\right) \label{eq:holt_level} \\
	b_T &= \beta^*\left( \ell_t - \ell_{t-1} \right) + (1 - \beta^*)b_{T-1} \label{eq:holt_trend}
\end{align}
\end_inset

where 
\begin_inset Formula $0 \leq \beta^* \leq 1$
\end_inset

. Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:holt_forecast"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:holt_level"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are the forecast and level equations as before. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:holt_trend"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the 
\shape italic
trend equation
\shape default
. The trend for a given timestep is a weighted sum of the change in the level beteen this time-step and the last time-step, and the previous trend. The level equation is influenced by the trend of the previous step. The forecast equation has a trending component.
\end_layout

\begin_layout Standard
Holt's linear method will give a forecast that is linear in the forecast horizon. A slightly better approach is to used the damped version. 
\begin_inset Formula \begin{align}
	\hat{y}_{T+h|T,\dots,1} &= \ell_T + \left( \phi + \phi^2 + \cdots + \phi^h \right)b_T \label{eq:holt_forecast}\\
	\ell_T &= \alpha y_T	+ (1 - \alpha)\left(\ell_{T-1} + \phi b_{T-1}\right) \label{eq:holt_level} \\
	b_T &= \beta^*\left( \ell_t - \ell_{t-1} \right) + (1 - \beta^*)\phi b_{T-1} \label{eq:holt_trend}
\end{align}
\end_inset

where 
\begin_inset Formula $0 < \phi < 1$
\end_inset

 is the damping coefficient. This method gives a forecast which is linear initially but approaches 
\begin_inset Formula \begin{align}
	\ell_T + \frac{\phi}{1-\phi} b_T \quad \text{ as } h \rightarrow \infty
\end{align}
\end_inset


\end_layout

\begin_layout Subsection
Holt-Winter's Seasonal Method
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:hw_seasonal"

\end_inset

 Introducing another smoothing equation can account for seasonal variations. The seasonal component can be additive or multiplicative. The additive version is, 
\begin_inset Formula \begin{align}
	\hat{y}_{T+h|T,\dots,1} &= \ell_T + hb_T + s_{T+h-m(k+1)} \label{eq:hw_forecast}\\
	\ell_T &= \alpha (y_T - s_{T-m}) + (1 - \alpha)\left(\ell_{T-1} + b_{T-1}\right) \label{eq:hw_level} \\
	b_T &= \beta^*\left( \ell_t - \ell_{t-1} \right) + (1 - \beta^*)b_{T-1} \label{eq:hw_trend} \\
	s_T &= \gamma \left( y_T - \ell_{T-1} - b_{T-1} \right) + \left( 1 - \gamma \right)s_{t-m} \label{eq:hw_seasonal}
\end{align}
\end_inset

where 
\begin_inset Formula $k$
\end_inset

 is the integer part of 
\begin_inset Formula $(h-1)/m$
\end_inset

, i.e. the number of seasons in the forecast horizon, and 
\begin_inset Formula $0 \leq \gamma \leq 1 - \alpha$
\end_inset

. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hw_seasonal"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the seasonal equation. This equation shows that the seasonal component is the weighted average of the seasonal index from this year 
\begin_inset Formula $(y_t - l_{t-1} - b_{t-1})$
\end_inset

 and last year 
\begin_inset Formula $s_{t-m}$
\end_inset

. The quantity 
\begin_inset Formula $y_t - s_t$
\end_inset

 is the 
\shape italic
deseasonalised series
\shape default
.
\end_layout

\begin_layout Standard
The multiplicative version is very similar 
\begin_inset Formula \begin{align}
	\hat{y}_{T+h|T,\dots,1} &= (\ell_T + hb_T)s_{T+h-m(k+1)} \label{eq:hw_forecast}\\
	\ell_T &= \alpha \frac{y_T}{ s_{T-m}} + (1 - \alpha)\left(\ell_{T-1} + b_{T-1}\right) \label{eq:hw_level} \\
	b_T &= \beta^*\left( \ell_t - \ell_{t-1} \right) + (1 - \beta^*)b_{T-1} \label{eq:hw_trend} \\
	s_T &= \gamma \frac{y_T}{\ell_{T-1} + b_{T-1}} + \left( 1 - \gamma \right)s_{t-m} \label{eq:hw_seasonal}
\end{align}
\end_inset

with the same resitrictions on the parameters. In this case, the deseasonalised series is 
\begin_inset Formula $y_t/s_t$
\end_inset

.
\end_layout

\begin_layout Standard
It's also possible to dampen the Holt-Winter's seasonal method in the same way as Holt's Linear method by introducing a damping parameter 
\begin_inset Formula $\phi$
\end_inset

.
\end_layout

\begin_layout Subsection
State Space Models
\end_layout

\begin_layout Standard
These expoential smoothing models are lacking an estimate for their error. When an error estimation is added to make these models complete, they are usually called 
\shape italic
state space
\shape default
 models.
\end_layout

\begin_layout Standard
Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ses_forecast"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ses_smoothing"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be re-written as 
\begin_inset Formula \begin{align}
	y_t &= \ell_{t-1} + \epsilon_t \label{eq:ss_measurement} \\
	\ell_t &= \ell_{t-1} + \alpha \epsilon_t \label{eq:ss_state}
\end{align}
\end_inset

where 
\begin_inset Formula $\epsilon_t = y_t - \ell_{t-1}$
\end_inset

, the 
\begin_inset Formula $t$
\end_inset

th residual. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ss_measurement"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the measurement equation and equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ss_state"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is called the state equation.
\end_layout

\begin_layout Standard
Errors can also be multiplicative. If 
\begin_inset Formula \begin{align}
	\epsilon_t = \frac{y_t - \hat{y}_{t|t-1,\dots,1}}{\hat{y}_{t|t-1,\dots,1}}
\end{align}
\end_inset

then the measurement and state equations become 
\begin_inset Formula \begin{align}
	y_t &= \ell_{t-1}(1 + \epsilon_t) \\
	\ell_t &= \ell_{t-1}(1 + \alpha \epsilon_t)
\end{align}
\end_inset

In both cases 
\begin_inset Formula $\epsilon \sim \mathcal{N}(0, \sigma^2)$
\end_inset

.
\end_layout

\begin_layout Standard
As an example Holt's linear method with additive errors is defined by 
\begin_inset Formula \begin{align}
	y_t &= \ell_{t-1} + b_{t-1} + \epsilon_t \\
	\ell_t &= \ell_{t-1} + b_{t-1} + \alpha \epsilon_{t} \\
	b_t &= b_{t-1} + \beta \epsilon_t 
\end{align}
\end_inset

where 
\begin_inset Formula $\epsilon \sim \mathcal{N}(0,\sigma^2)$
\end_inset

, and 
\begin_inset Formula $\beta = \alpha\beta^*$
\end_inset

.
\end_layout

\begin_layout Subsection
ETS(
\begin_inset Formula $\cdot,\cdot,\cdot$
\end_inset

)
\end_layout

\begin_layout Standard
State space models are often denoted by the method error, trend, and seasonality used. The type of error can be additive, 
\begin_inset Formula $A$
\end_inset

, or multiplicative 
\begin_inset Formula $M$
\end_inset

. The type of trend can be none 
\begin_inset Formula $N$
\end_inset

, additive 
\begin_inset Formula $A$
\end_inset

, and additive but dampened 
\begin_inset Formula $A_d$
\end_inset

. The type of seasonality can be none, additive, or multiplicative. There are 18 different types of state space model. They are all summarised in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ets_models"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/ets_models.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
All possible ETS models. From 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "fpp"
literal "true"

\end_inset

.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:ets_models"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Modelling the Volatility: ARCH, GARCH
\end_layout

\begin_layout Standard
Until this section, all of the models mentioned in this document have noise with a constant variance. The models in this section are used for modelling the variance in the noise. ARCH stands for auto-regressive conditional heteroskedasticity. The `skedasticity' part of the heteroskedasticity comes from the Greek 
\shape italic
skedastikos
\shape default
, meaning `able to disperse'. A 
\shape italic
homoskedastic
\shape default
 variable is dispersed by the same amount all the time, i.e. with constant variance, whereas a 
\shape italic
heteroskedastic
\shape default
 variable has a variance that changes over time.
\end_layout

\begin_layout Standard
An ARCH model models the variance of the noise as an auto-regressive variable. If 
\begin_inset Formula $\epsilon_t$
\end_inset

 is the noise of the model, say 
\begin_inset Formula \begin{align}
	\epsilon_t = \sigma_t z_t
\end{align}
\end_inset

where 
\begin_inset Formula $z_t \sim \mathcal{N}(0,1)$
\end_inset

 is white noise, and 
\begin_inset Formula \begin{align}
	\sigma^2_t = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
\end{align}
\end_inset

is an 
\begin_inset Formula $AR(q)$
\end_inset

 model for the variance of the noise. When an 
\begin_inset Formula $AR(q)$
\end_inset

 model is used to model the volatility of a time series, the volatility at any given time step will be correlated with the previous steps. If the volatility is modelled using white noise, the volatility will be uncorrelated.
\end_layout

\begin_layout Standard
If an 
\begin_inset Formula $ARMA(q,p)$
\end_inset

 is used to model the volatility, this is called a generalised auto-regressive conditional heteroskedasticity model, or GARCH model. In this case 
\begin_inset Formula \begin{align}
	\sigma^2_t = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \beta_i \sigma^2_{t-i}
\end{align}
\end_inset

where 
\begin_inset Formula $\omega = \alpha_0 + \beta_0$
\end_inset

. Introducing the moving average model part allows the model to quickly respond to sudden changes in the volatility. The auto-regressive part then propagates these changes forward.
\end_layout

\begin_layout Standard

\begin_inset Formula $ARCH(q)$
\end_inset

 and 
\begin_inset Formula $GARCH(q,p)$
\end_inset

 models are popular in financial forecasting since the volatility is rarely white noise, and forecasting volatility accurately can be profitable. The 
\begin_inset Formula $GARCH(1,1)$
\end_inset

 model is most popular in this context.
\end_layout

\begin_layout Section
Ensemble Methods
\end_layout

\begin_layout Standard
Apply several simpler models and get many forecasts. Then aggregate these forecasts and their prediction intervals in one way or another. Seems like this always improves the overall performance.
\end_layout

\begin_layout Section
Meta-learning
\end_layout

\begin_layout Standard
Meta-learning in forecasting refers to the process of learning which of a collection of forecasting methods is most suitable to use in a given situation, or learning how to combine a collection of methods into an ensemble approach. When building a meta-learning framework, there are three distinct steps that are usually necessary. 
\end_layout

\begin_layout Subsection
Building the Meta-learning Model
\end_layout

\begin_layout Subsubsection
Augmenting the Dataset
\end_layout

\begin_layout Standard
Adding to the dataset with simulated time series. In order to take the machine learning approaches necessary in meta-learning, a large and balanced time series dataset is required. Therefore it can be necessary to augment whole the dataset, or augment specific subsets of the dataset in order have an equal number of examples of each type of time series. When simulating time series, care should be taken to create series which have similar characteristics to the observed time series. Time series can be simulated using pre-defined functions in 
\family typewriter
R
\family default
 such as 
\family typewriter
auto.arima
\family default
 or 
\family typewriter
ets
\family default
.
\end_layout

\begin_layout Standard
Additional time series can also be created by bootstrapping, that is sampling chunks from existing time series. When deciding what size the sampled chunks should be, the period of the data's cycles and seasonality must be taken into account. If the bootstrapped data will be used to train an RNN that uses LSTMs or GRUs, any long term dependencies must be taken into account.
\end_layout

\begin_layout Subsubsection
Feature Extraction
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:features"

\end_inset

 Feature extraction. In order to train the meta-learning model, features must be extracted from each of the time series and included in the training set along with the time series itself. Examples of features used by meta-learning models are: first autocorrelation coefficient, first autocorrelation coefficient of the first differenced series, first autocorrelation of the twice-differenced series, spectral entropy, the 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 parameter of an 
\begin_inset Formula $ets(A,A,N)$
\end_inset

 model, 
\shape italic
lumpiness
\shape default
, seasonal period, trend strength from an STL decomposition, series length, etc., etc. It is normal to extract dozens of different features from each time series. The time series combined with the features extracted from that series make up one training example.
\end_layout

\begin_layout Subsubsection
Training
\end_layout

\begin_layout Standard
The first step to training the model is to apply each of the forecasting methods to each time series. Then to measure the accuracy of each forecast. Then to choose a loss function for the meta-learning model, the minimisation of which will minimise the forecasting error of the meta-learning model. Then to use some optimisation algorithm to find the meta-learning parameters that minimise the loss function. In the case where the model combines the collection of forecasting methods, another simpler model will be required to calculate prediction intervals, and that model must be trained in a similar way.
\end_layout

\begin_layout Subsection
Using the Meta-learning Model
\end_layout

\begin_layout Standard
Building the meta-learning model is usually done offline before the model is used to forecast new time series. This means that as long as the time taken to train the model is reasonable (less than two days for example) this time can be disregarded, and forecasts can be obtained very quickly.
\end_layout

\begin_layout Standard
In order to obtain a forecast from the meta-learning model, the features mentioned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:features"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are extracted. Then the features and the time series itself are fed in to the model, and the model will return a forecast and prediction intervals. It's 
\series bold
that
\series default
 easy!
\end_layout

\begin_layout Subsection
Examples
\end_layout

\begin_layout Subsubsection
Feature-based Forecast-Model Selection (FFORMS)
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "talagala"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Talagala et al (2018) trained a random forest to classify the best forecasting method for a given time series out of a collection of ARIMA and ETS models. They used the datasets of the M1 and M3 competitions, and augmented these datasets using ARIMA and ETS models to simulate new time series. They extracted thirty-three different features from each time series. They trained a different classifier for yearly, quarterly, and monthly data from the M1 and M3 competitions. The FFORMS model consistently ranks in the top most accurate forecasting methods for forecasting both the M1 and the M3 series and most often ranks as the most accurate method.
\end_layout

\begin_layout Standard
A schematic diagram of the FFORMS framework is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FFORMS"
plural "false"
caps "false"
noprefix "false"

\end_inset

. This diagram is general enough to give a good understanding of a generalised meta-learning framework.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/FFORMS.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic diagram of the FFORMS framework. The offline phase is shown in red, the online/training phase is shown in blue.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:FFORMS"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Meta-weighted Ensemble
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "hyndman"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
Montero-Manso et al (2018) trained a gradient boosted tree model to learn the weights for a weighted ensemble of simple forecasting methods, and entered their model in the M4 forecasting competition. They used the M4 competition dataset. No augmentation was necessary. While training the model they performed a search over the hyper-parameter space of the gradient boosted tree model, using cross-validation to find the best hyper-parameters. They extracted forty-two features from each of the time series in training.
\end_layout

\begin_layout Standard
To use the model for any new time series, the features are extracted and the forecasts of the simple methods are calculated. The model then takes the time series and the features and outputs a vector of real numbers the same length as the number of forecasting methods. These numbers are transformed into probabilities using a softmax function.
\end_layout

\begin_layout Standard
The 
\shape italic
softmax
\shape default
 function is a squashing function which can take an arbitrary vector of real numbers 
\begin_inset Formula $\mathbf{x} \in \mathbb{R}^n$
\end_inset

 and transform it into a vector of values between 
\begin_inset Formula $0$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

 that sum to 
\begin_inset Formula $1$
\end_inset

. Formally, 
\begin_inset Formula \begin{align}
	\sigma : \mathbb{R}^n \rightarrow& \left\{ \sigma(\mathbf{x}) \in \mathbb{R}^n : \sigma(\mathbf{x})_j > 0 \forall j, \sum_{j=1}^n \sigma(\mathbf{x})_j = 1 \right\} \\
	\sigma(\mathbf{x})_i &= \frac{e^{\mathbf{x}_i}}{\sum_{j=1}^n e^{\mathbf{x}_j}}
\end{align}
\end_inset

The softmax function is used in multi-class classification where a linear regression model is passed through a softmax function to give categorical probabilities. In this case there is one probability for each of the forecasting methods. These probabilities are used to combine the forecasts linearly in order to produce a single forecast.
\end_layout

\begin_layout Standard
In order to calculate prediction intervals, the model calculates the prediction intervals radius of a subset of the forecasting methods. A subset is taken for computational cost reasons. These radii are combined linearly using a weight matrix which is learned in training to give an upper and lower value for the interval for each time step in the forecasting horizon.
\end_layout

\begin_layout Section
Time Series Features
\end_layout

\begin_layout Subsection
Spectral Entropy
\end_layout

\begin_layout Subsubsection
Shannon's Entropy
\end_layout

\begin_layout Standard
Shannon's Entropy aka. Information Entropy is a measure of the unpredictability of a random variable. If 
\begin_inset Formula $X$
\end_inset

 is a random variable with possbile values 
\begin_inset Formula ${x_1, \dots, x_N}$
\end_inset

 and associated probabilities 
\begin_inset Formula $P(X=x_1), \dots, P(X=x_N)$
\end_inset

, the entropy of the random variable 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula \begin{align}
	H(X) 	&= -\sum_{n=1}^N P(X = x_n)\log P(X = x_n) \label{eq:entropy}\\
		&= \sum_{n=1}^N P(X = x_n) \log \frac{1}{P(X=x_n)}
\end{align}
\end_inset

The entropy of a continuous random variable is defined similary, but with integrals instead of summations.
\end_layout

\begin_layout Standard
If the variable is uniformly distributed across its values, then its entropy is maximised. If 
\begin_inset Formula $X$
\end_inset

 only ever takes one of its possible values, then 
\begin_inset Formula $H(X)=0$
\end_inset

. Usually the 
\begin_inset Formula $\log$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:entropy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is taken to the base 
\begin_inset Formula $2$
\end_inset

, and the entropy is therefore measured in 
\shape italic
bits
\shape default
.
\end_layout

\begin_layout Subsubsection
Frequency Representation of Time Series
\end_layout

\begin_layout Standard
Any time series can be expressed as a function of freqencies instead of as a function of time. The Fourier transformation transforms a time series into its frequency representation 
\begin_inset Formula \begin{align}
	x(f) = \int_{-\infty}^{\infty} e^{-2\pi i f t}x(t) dt
\end{align}
\end_inset

The 
\shape italic
power spectrum
\shape default
 of the time series is 
\begin_inset Formula $S(f) = \modu{x(f)}^2$
\end_inset

. The normalised power spectrum is 
\begin_inset Formula \begin{align}
	P(f) = \frac{S(f)}{\sum_{f^{\prime}}S(f^{\prime})}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
The normalised power spectrum of a time series takes the form of a probability distribution and therefore defines a random variable over the frequencies 
\begin_inset Formula $f^{\prime}$
\end_inset

. The information entropy of this random variable is the 
\shape italic
spectral entropy
\shape default
 of the time series.
\end_layout

\begin_layout Subsection
Stability & Lumpiness
\end_layout

\begin_layout Standard
If a time series of length 
\begin_inset Formula $L$
\end_inset

 is divided up into non-overlapping windows of 
\begin_inset Formula $\tau$
\end_inset

 time-steps, and a variance is measured from the data within each window, then the 
\shape italic
stability
\shape default
 is the mean of the variances, and the 
\shape italic
lumpiness
\shape default
 is the variance of the variances. There are 
\family typewriter
R
\family default
 functions for both of these quantities.
\end_layout

\begin_layout Section
Recurrent Neural Networks
\end_layout

\begin_layout Standard
Recurrent neural networks (RNNs) are linear combinations of non-linear 
\shape italic
activation
\shape default
 functions applied to inputs combined in a way that is specifically designed for performing calculations on sequences, including time series.
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:forward_propagation"
plural "false"
caps "false"
noprefix "false"

\end_inset

 a schematic diagram of a simple recurrent neural network is shown. The 
\begin_inset Formula $x$
\end_inset

s are the inputs, for example a given time series. The 
\begin_inset Formula $a$
\end_inset

s are the activation values. The 
\begin_inset Formula $y$
\end_inset

s are the outputs. In this example network, the activations and outputs are defined by 
\begin_inset Formula \begin{align}
	a^{\langle t \rangle} &= f(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a) \\
	&= f(W_{a}[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_a) \\
	y^{\langle t \rangle} &= g(W_{ya}a^{\langle t \rangle} + b_y)
\end{align}
\end_inset

where 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

 are some non-linear 
\shape italic
activation functions
\shape default
, such as 
\begin_inset Formula $\tanh$
\end_inset

, or the sigmoid function. There are a lot of possible variations to the network architecture. The output for step 
\begin_inset Formula $t$
\end_inset

 could be used as part of the input to step 
\begin_inset Formula $t+1$
\end_inset

 for example, or there could be connections that skip over one or more time steps, or outputs may not be produced until after all the inputs have been processed. The activation functions need not be just one function, there could be several functions connected to make up a smaller network which gives the activation function.
\end_layout

\begin_layout Standard
For choosing activation functions there are a lot of potential functions from which to choose. There are some characteristics that would be beneficial for an activation function. 
\end_layout

\begin_layout Description
Non-linearity It has been proven that a two layer neural network with non-linear activation functions can approximate any function. Also, to capture non-linear dynamics requires non-linear functions. 
\end_layout

\begin_layout Description

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
Continuously Differentiable
\end_layout

\end_inset

 This makes gradient based optimisation easier. 
\end_layout

\begin_layout Description
Monotonicity If the activation function is monotonic, the error surface for a one-layer neural network is guaranteed to be convex. Meaning that there is only one optimum point, no local optima where an optimisation algorithm can get stuck. 
\end_layout

\begin_layout Description

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
Smooth (Infinitely Differentiable) functions with a Monotonic derivative
\end_layout

\end_inset

 These have been shown to generalised better in some cases (apparently). 
\end_layout

\begin_layout Description

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
Approximates the identity function near the origin
\end_layout

\end_inset

 If the activity function looks like the identity function 
\begin_inset Formula $f(x) = x$
\end_inset

 near the origin, then the network will learn efficiently when the paramters are initialised to small values. So this gives a good starting point for optimisation. If 
\begin_inset Formula $f(0) = 0$
\end_inset

, 
\begin_inset Formula $f^{\prime}(0) = 1$
\end_inset

, and 
\begin_inset Formula $f^{\prime}$
\end_inset

 is continuous around 0, then it's fair to say that 
\begin_inset Formula $f$
\end_inset

 approximates the identity around the origin. 
\end_layout

\begin_layout Standard
The range or codomain of the function will have an effect on the network also. If the range of the function is restricted to some interval then gradient descent optimisation methods tend to be stable as the value of the gradient is very small for most values. On the other hand, the training process will suffer from the vanishing gradient problem causing learning to slow down as training goes on. 
\begin_inset Formula $\tanh$
\end_inset

 and the sigmoid function are examples of activation functions with ranges restricted to an interval. If the range of the function is unrestricted, training tends to be more efficient as the gradient of the activation function doesn't disappear, but a smaller learning rate may be necessary to find the optimum. The rectified linear unit and any other activation fuctions like that have unrestricted ranges. 
\begin_inset Foot
status collapsed


\begin_layout Standard
A fairly comprehensive list of candidate activation functions can be found here: 
\begin_inset Flex Flex:URL
status collapsed

\begin_layout Plain Layout
https://en.wikipedia.org/wiki/Activation_function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/forward_propagation.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic diagram of a simple recurrent neural network. The 
\begin_inset Formula $x$
\end_inset

s are inputs, the 
\begin_inset Formula $a$
\end_inset

s are the activations, and the 
\begin_inset Formula $y$
\end_inset

s are outputs.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:forward_propagation"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The backpropagation algorithm is used to train RNNs, just like any neural network. If the gradient of the loss function is propagated back through all of the time steps in the input sequence, the gradient may become exponentially small, or exponentially large. This is called the vanishing gradient problem, or exploding gradient problem, respectively. A vanishing gradient will make it difficult for information or errors (and error correction) to propagate along the RNN and will inhibit learning over time. An exploding gradient will make learning impossible as the gradient becomes too large to compute.
\end_layout

\begin_layout Standard
The vanishing gradient problem is more common in RNNs, and the exploding gradient problem is easier to deal with. Therefore, many of the innovations in RNNs are attempts to solve the vanishing gradient problem. The most well known and widely used of these is the 
\shape italic
long-short term memory unit
\shape default
, or LSTM (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:lstm"
plural "false"
caps "false"
noprefix "false"

\end_inset

). There have been some innovations on the LSTM idea. The Gated Recurrent Unit (GRU) and the Just Another Net (JANET) will be covered here.
\end_layout

\begin_layout Standard
The LSTM, GRU, and JANET are examples of `cells' or `neurons' that can be stacked and connected in order to build the RNN. There have also been innovations in the way the RNNs are connected, i.e. the 
\shape italic
architecture
\shape default
 of the network.
\end_layout

\begin_layout Subsection
Cell Innovations: LSTM, GRU, JANET
\end_layout

\begin_layout Subsubsection
Long-Short Term Memory Unit (LSTM)
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:lstm"

\end_inset

 Each of the neurons in the RNN can take the form of an LTSM. So the LSTM will take an input 
\begin_inset Formula $x^{\langle t \rangle}$
\end_inset

 and an activation 
\begin_inset Formula $a^{\langle t-1 \rangle}$
\end_inset

 from the previous time step, and will output an activation. The activation could then be used to create an output for the RNN. The LSTM is defined by 
\begin_inset Formula \begin{align}
	\tilde{c}^{\langle t \rangle} &= \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) \label{eq:lstm_cand} \\
	\Gamma_u &= \sigma(W_u[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u) \label{eq:lstm_update} \\
	\Gamma_f &= \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f) \label{eq:lstm_forget} \\
	\Gamma_o &= \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o) \label{eq:lstm_output} \\
	c^{\langle t \rangle} &= \Gamma_u \odot \tilde{c}^{\langle t \rangle} + \Gamma_f \odot c^{\langle t-1 \rangle} \label{eq:lstm_cell} \\
	a^{\langle t \rangle} &= \Gamma_o \odot \tanh( c^{\langle t \rangle}) \label{eq:lstm_act}
\end{align}
\end_inset

where 
\begin_inset Formula $\odot$
\end_inset

 represents element-wise multiplication and 
\begin_inset Formula $\sigma$
\end_inset

 represents the sigmoid function. The 
\begin_inset Formula $c^{\langle t \rangle}$
\end_inset

 quantity can be interpreted as an internal value for the LSTM. The LSTM is sometimes referred to as a `cell', so the letter `c' is used. The 
\begin_inset Formula $\tilde{c}^{\langle t \rangle}$
\end_inset

 quantity can be interpreted as a candidate for the updated internal value which is calculated using the current input 
\begin_inset Formula $x^{\langle t \rangle}$
\end_inset

 and the activation of the previous time-step 
\begin_inset Formula $a^{\langle t-1 \rangle}$
\end_inset

 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cand"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_update"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_forget"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_output"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are the 
\shape italic
update
\shape default
, 
\shape italic
forget
\shape default
, and 
\shape italic
output
\shape default
 gates respectively. The first two of these gates together with equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 control the update to the internal value of the cell. The updated internal value 
\begin_inset Formula $c^{\langle t \rangle}$
\end_inset

 will be a combination of the candidate update and the previous internal value 
\begin_inset Formula $c^{\langle t-1 \rangle}$
\end_inset

. The activation of the unit 
\begin_inset Formula $a^{\langle t \rangle}$
\end_inset

 is a non-linear function of the updated internal value scaled by the output gate. A schematic diagram of an LSTM can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lstm"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/lstm.png

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic diagram of an LSTM.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:lstm"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The whole network of the LSTM and in particular equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 allows for errors, and therefore gradients, to propagate back through the network without diminishing in size. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines a state within the cell that may not be affected by incoming inputs and activations depending on the update and forget gates. Since the update and forget gates are defined using the sigmoid function, these gates must be between 
\begin_inset Formula $0$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

. Furthermore, the value of these gates is likely to be pushed towards 0 or 1 by the back propagation process. So, these gates can be intuitively interpreted as a vector of binary values of the same length as the input. Therefore the parameters 
\begin_inset Formula $W_u$
\end_inset

 and 
\begin_inset Formula $W_f$
\end_inset

 should allow the RNN to learn when the value of a certain input or activation will have an effect on an output much later on in the input time series. This is the `memory' referred to in the title of the LSTM.
\end_layout

\begin_layout Subsubsection
Gated Recurrent Unit (GRU)
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "cho, chung"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The GRU can be seen as a simplification of the LSTM because it avoids the vanishing gradient function in the same way as the LSTM, but using fewer equations and fewer parameters. Similar to the LSTM, the GRU uses an internal value or `state' which might be updated slightly or updated a lot at every time step. But the GRU uses this state as its activation, so there is no need for an output gate. The GRU also disregards the forget gate, and uses a single update gate to decide by how much the internal state will be updated. The only addition to the LSTM in the GRU is a 
\shape italic
relevance gate
\shape default
, which is designed to capture the relevance of the previous state (combined with the input) to the candidate state.
\end_layout

\begin_layout Standard

\begin_inset Formula \begin{align}
	\Gamma_r &= \sigma(W_r[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_r) \label{eq:gru_rel} \\
	\tilde{c}^{\langle t \rangle} &= \tanh(W_c[\Gamma_r \odot c^{\langle t \rangle},x^{\langle t \rangle}] + b_c) \label{eq:gru_cand} \\
	\Gamma_u &= \sigma(W_u[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u) \label{eq:gru_update} \\
	c^{\langle t \rangle} &= \Gamma_u \odot \tilde{c}^{\langle t \rangle} + (1 - \Gamma_u) \odot c^{\langle t-1 \rangle} \label{eq:gru_cell}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gru_rel"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is the relevance gate, mentioned above. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gru_cand"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines the candidate state. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gru_update"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines the update gate, similar to the LSTM. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gru_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines the the updated internal state. A schematic diagram of a GRU is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gru"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/gru.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic diagram of a GRU.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:gru"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
GRUs have been found to be comparable to LSTMs in certain circumstances and to outperform LSTMs on smaller datasets 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "chung"
literal "true"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Just Another Net (JANET)
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "janet"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The JANET is another simplification of the standard LSTM. Van der Westhuizen et al (2018) removed the update and output gates from the LSTM, leaving only the forget gate. They also subtracted a constant from the argument for the forget gate when acting on the candidate state. The idea being that this would make it easier for information to accumulate in the cell's memory. The JANET is defined by 
\begin_inset Formula \begin{align}
	s^{\langle t \rangle} &= W_f[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f \label{eq:janet_in} \\
	\tilde{c}^{\langle t \rangle} &= \tanh(W_c[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) \label{eq:janet_cand} \\
	c^{\langle t \rangle} &= \sigma(s^{\langle t \rangle}) \odot c^{\langle t-1 \rangle} + (1 - \sigma(s^{\langle t \rangle} - \beta)) \odot \tilde{c}^{\langle t \rangle} \label{eq:janet_cell}
\end{align}
\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:janet_in"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines the 
\shape italic
input control component
\shape default
. This would usually be the argument of the sigmoid function in the forget gate. But the forget gate takes a slightly different form here, so it is easier to define the cell this way. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:janet_cand"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is the usual candidate definition. Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:janet_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 defines the updated internal state. The first term of this equation is standard and controls how much the remembered state contributes to the updated state. The second term is standard except for the subtraction of the 
\begin_inset Formula $\beta$
\end_inset

. This subtraction reduces the contribution of the candidate state to the updated state, which helps the JANET's `memory' accumulate. In 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "janet"
literal "true"

\end_inset

, the authors found that setting 
\begin_inset Formula $\beta=1$
\end_inset

 gave the best results on the tests performed. Similar to the GRU, the JANET uses the updated state 
\begin_inset Formula $c^{\langle t \rangle}$
\end_inset

 as its activation. A schematic diagram of a JANET is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:janet"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/janet.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Schematic diagram of a JANET.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:janet"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The process of 
\shape italic
chrono initialization
\shape default
 detailed in 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "tallec"
literal "true"

\end_inset

 was used to improve the JANET's performance. This amounts to initialising the bias of the forget gate 
\begin_inset Formula $b_f$
\end_inset

 to a 
\shape italic
characteristic forgetting time
\shape default
, which is defined by the longest time dependency that the network will have to capture. This dependency is defined by the user when setting up the network.
\end_layout

\begin_layout Standard
Van der Westhuizen et al tested the JANET using some standard tests for RNNs such as testing the performance on the MNIST dataset, and the permuted MNIST dataset. They found that the JANET outperformed the standard LSTM on all the tests performed. A JANET was also used as part of the CFM competition to predict future volatility based on past volatility. The user reported better performance than a regualr LSTM, while using fewer parameters 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "cfm"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
N.B.:
\series default
 The JANET was outperformed by something called the 
\shape italic
tensorised LSTM (tLSTM)
\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "he"
literal "true"

\end_inset

. May be worth looking into.
\end_layout

\begin_layout Subsection
Architectural Innovations
\end_layout

\begin_layout Subsubsection
Dilated Recurrent Neural Networks 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "chang"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
A skip connection in a recurrent neural network is an extra connection between two time steps a number of time steps apart. The normal connections in the RNN are still there. A dilated recurrent skip connection is a connection between steps in a recurrent neural network that skips one or more of the next steps without the normal one-step connections. A schematic diagram of a dilated skip connection can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dilated_connection"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as compared to a standard skip connection. This has the effect of reducing the number of layers through which an error must be back propogated, which addresses the vanishing gradient problem, reduces the number of parameters in the network, and forces the network to learn some historical dependencies. The dilated skip connections also allow calculations to be performed in parallel, thereby exploiting the parallelising power of a GPU.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/dilated_skip_connection.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
(left) A one layer recurrent neural network with a skip connections. (middle) A one layer recurrent neural network with dilated recurrent skip connections. (right) A structure equivalent to the middle network. The input sequence length is reduced by four, and each of the four sequences can be processed in parallel.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dilated_connection"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If the activation of a cell in layer 
\begin_inset Formula $l$
\end_inset

 and time 
\begin_inset Formula $t$
\end_inset

 of an RNN with standard skip connections is denoted by 
\begin_inset Formula $c^{\langle t \rangle(l)}$
\end_inset

, then 
\begin_inset Formula \begin{align}
	a^{\langle t \rangle(l)} = f\left(x^{\langle t \rangle(l)}, a^{\langle t - 1 \rangle(l)}, a^{\langle t - s^{(l)} \rangle(l)}\right)
\end{align}
\end_inset

The activation of a similar cell in an RNN with dilated skip connections is 
\begin_inset Formula \begin{align}
	a^{\langle t \rangle(l)} = f\left(x^{\langle t \rangle(l)}, a^{\langle t - s^{(l)} \rangle(l)}\right)
\end{align}
\end_inset

where 
\begin_inset Formula $s^{(l)}$
\end_inset

 is the length of the skip connection in layer 
\begin_inset Formula $l$
\end_inset

, 
\begin_inset Formula $x^{(l)}$
\end_inset

 is the input to layer 
\begin_inset Formula $l$
\end_inset

, and 
\begin_inset Formula $f$
\end_inset

 is a function that could represent any linear or non-linear function or collection of functions commonly used in an RNN, including LSTM, GRU, etc.
\end_layout

\begin_layout Standard
A dilated recurrent neural network is one where dilated layers are stacked together in order to form a complete network. The dilation in each layer should increase exponentially. This makes different layers focus on different temporal resolutions, and reduces the average length of paths between nodes at different time points (proof in 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "chang"
literal "true"

\end_inset

). An example dilated RNN with three layers and a dilation of two can be seen in figure
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/dilated_rnn.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of a dilated RNN with three layers with dilations of 1, 2, and 4, respectively.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dilated_rnn"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The dilations in a dilated RNN can cause missing dependencies in the output layer. These dependencies can be reintroduced by including convolutional connections from the final hidden layer to the output layer.
\end_layout

\begin_layout Standard
In order to test the performance of the dilated RNN, Chang et al uses the 
\shape italic
copy memory problem
\shape default
. This task tests the ability of recurrent models to memorise long-term information. Each input sequence is of length 
\begin_inset Formula $T+ 20$
\end_inset

. The first ten values are randomly generated from integers 0 to 7, the next 
\begin_inset Formula $T-1$
\end_inset

 values are all 8, the last 11 values are all 9. The first occurance of a 9 tells the model that it must reproduce the first 10 digits. The random guess yields an expected average cross entropy of 
\begin_inset Formula $\ln(8) \approx 2.079$
\end_inset

. The results of the copy memory problem can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:copy_memory_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/copy_memory_problem.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Results of the copy memory problem with 
\begin_inset Formula $T= 500$
\end_inset

 (left) and 
\begin_inset Formula $T = 1000$
\end_inset

 (right). Only RNNs with dilated skip connections converge to the perfect solution, all other methods are unable to improve over random guesses. 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:copy_memory_problem"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Chang et al also used the MNIST dataset of handwritten digits to test the performance of various RNNs. The RNN was tasked with classifying the digit after reading in each of the pixels one-by-one as a 
\begin_inset Formula $784 \times 1$
\end_inset

 sequence. The results of the task can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dilated_mnist"
plural "false"
caps "false"
noprefix "false"

\end_inset

. All the RNNs without skip connections failed at the task. The `Vanilla' RNN with standard skip connections showed some success. This is consistent with the finding that RNNs with skip connections are better at learning long term dependencies. The RNNs with dilated skip connections performed the best, with the dilated GRU showing the best performance after training.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/dilated_mnist.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Results of the MNIST classification task used by Chang et al to assess the performance of the dilated RNNs against other RNNs. The dilated RNNs perform the best, with the dilated GRU showing the best performance after training.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dilated_mnist"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
The Attention Mechanism 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "bahdanau"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The attention mechanism deals with the case where there exists a target time series 
\begin_inset Formula $y_1, \dots, y_{t-1} \in \mathbb{R}$
\end_inset

 and 
\begin_inset Formula $n$
\end_inset

 driving (exogeneous) series 
\begin_inset Formula $\mathbf{x}_1, \dots, \mathbf{x}_t \in \mathbb{R}^n$
\end_inset

, and the objective is to learn a function 
\begin_inset Formula $F$
\end_inset

 such that 
\begin_inset Formula $\hat{y}_t = F(y_1, \dots, y_{t-1}, \mathbf{x}_1, \dots, \mathbf{x}_t)$
\end_inset

.
\end_layout

\begin_layout Standard
The first step is encoding the driving series 
\begin_inset Formula $(\mathbf{x}_1, \dots, \mathbf{x}_t) = \mathbf{X} \in \mathbb{R}^{n \times T}$
\end_inset

. This is done using a bi-directional RNN. If 
\begin_inset Formula $\mathbf{X}^{\top} \in \mathbb{R}^{T \times n}$
\end_inset

 is a time series of vectors containing one element from each of the 
\begin_inset Formula $n$
\end_inset

 driving series, then a bi-directional RNN will read forward from 
\begin_inset Formula $\mathbf{x}_1, \dots,\mathbf{x}_{T_x}$
\end_inset

 calculating foward hidden states 
\begin_inset Formula $\overrightarrow{h}_1, \dots, \overrightarrow{h}_{T_x}$
\end_inset

, then read backward from 
\begin_inset Formula $\mathbf{x}_{T_x}, \dots,\mathbf{x}_1$
\end_inset

 calculating backward hidden states 
\begin_inset Formula $\overleftarrow{h}_{T_x}, \dots, \overleftarrow{h}_1$
\end_inset

. Concatenating these forward and backward hidden states together gives an encoding, also known as an annotation, 
\begin_inset Formula $h_{t}$
\end_inset

 which contains information from the whole time series of vectors, but is focussed around timestep 
\begin_inset Formula $t$
\end_inset

. It is common to use LSTMs or GRUs for these encoders.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/attention_mechanism.png
	width 50text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A schematic diagram of an RNN with an attention mechanism. The inputs 
\begin_inset Formula $\mathbf{X}_t$
\end_inset

 are encoded by a bi-directional RNN into hidden states 
\begin_inset Formula $h_t$
\end_inset

. Those hidden states are combined with the decoder RNN hidden state 
\begin_inset Formula $s_t$
\end_inset

 in the attention model to create a context vector. The context vector is combined with the previous output, and the previous hidden state to create the next output.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:attention_mechanism"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output of the network will come from a forward reading RNN, that reads in the last output 
\begin_inset Formula $y_{t-1}$
\end_inset

, the current hidden state 
\begin_inset Formula $s_t$
\end_inset

, and the current 
\shape italic
context vector
\shape default
 
\begin_inset Formula $c_t$
\end_inset

. To calculate the context vector, the last hidden state of the output RNN is combined with the encoder hidden state corresponding to the current timestep 
\begin_inset Formula $h_{t^{\prime}}$
\end_inset

 in an 
\shape italic
alignment model
\shape default
, 
\begin_inset Formula \begin{align}
	e_{tt^{\prime}} = a(s_{t-1}, h_{t^{\prime}})
\end{align}
\end_inset

In 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "bahdanau"
literal "true"

\end_inset

, the alignment model 
\begin_inset Formula $a$
\end_inset

 takes the form of a feed-forward neural network that is trained along with the rest of the network. The 
\begin_inset Formula $e_{tt^{\prime}}$
\end_inset

 are combined to create attention coefficients, or attention weightings using the softmax function 
\begin_inset Formula \begin{align}
	\alpha_{tt^{\prime}} = \frac{\exp(e_{tt^{\prime}})}{\sum_{v=1}^{T_x} \exp(e_{tv})}
\end{align}
\end_inset

In the machine translation context 
\begin_inset Formula $\alpha_{tt^{\prime}}$
\end_inset

 is a probability that the target word 
\begin_inset Formula $y_t$
\end_inset

 is aligned to, or translated from, a source word 
\begin_inset Formula $x_{t^{\prime}}$
\end_inset

. These attention weightings are used to combine the hidden states linearly to make the context vector, 
\begin_inset Formula \begin{align}
	c_t = \sum_{v=1}^{T_x}\alpha_{tv}h_v
\end{align}
\end_inset

The context vector, the previous hidden state, and the previous output is combined to create the current hidden state of the output RNN 
\begin_inset Formula \begin{align}
	s_t = f(s_{t-1}, y_{t-1}, c_t)
\end{align}
\end_inset

and this hidden state is combined with the previous output and the context vector to create the conditional probability of the next output 
\begin_inset Formula \begin{align}
	P(y_t | y_{1},\dots,t_{t-1}, \mathbf{X}) = g(y_{t-1}, s_t, c_t)
\end{align}
\end_inset

The next output 
\begin_inset Formula $y_t$
\end_inset

 is sampled from this distribution. A schematic diagram of the attention mechanism, aka the attention model is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:attention_mechanism"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
The point here is to encode the amount of attention that should be paid to input 
\begin_inset Formula $\mathbf{x}_{t^{\prime}}$
\end_inset

 when calculating output 
\begin_inset Formula $y_t$
\end_inset

. This information is held in the attention weightings 
\begin_inset Formula $\alpha_{tt^{\prime}}$
\end_inset

 where 
\begin_inset Formula $t,t^{\prime} \in {1, \dots, T}$
\end_inset

. The attention weightings can be displayed as a kind of cross correlation matrix in order to discover where dependencies lie. The encoder-decoder architecture also allows the output length to vary regardless of the input length.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "bahdanau"
literal "true"

\end_inset

, Bahdanau et al tested an RNN encoder-decoder with attention mechanism on English to French translation. A corpus of 348 million words was used. The BLEU score was used to asses the quality of the translation. The BLEU score assesses how well machine translation matches human translation, the closer the match, the higher the score. 
\begin_inset Foot
status collapsed


\begin_layout Standard
For more information on the BLEU score see: 
\begin_inset Flex Flex:URL
status collapsed

\begin_layout Plain Layout
https://en.wikipedia.org/wiki/BLEU
\end_layout

\end_inset


\end_layout

\end_inset

 The RNN encoder-decoder with the attention mechanism (referred to as RNN-search) performance was compared to the performance of an RNN encoder-decoder without the attention mechanism (referred to as RNN-enc). Each type of model was trained using sentences of length up to 30 words, and then sentences of length up to 50 words. The results of the translation task are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:attention_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The network with the attention mechanism significantly outperformed the network without. Even the attention network trained with sentences up to 30 words in length outperformed the attention-less network trained with sentences up to 50 words in length when translating sentences up to 60 words in length.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/attention_results.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
The RNN encoder-decoder with attention (RNN-search) and the RNN encoder-decoder without attention (RNN-enc) were used to translate English into French, and then assessed using the BLEU score system. Each type of model was trained with sentences up to 30 words in length, then up to 50 words in length. The RNN with the attention mechanism significantly outperformed the RNN without the attention mechanism, particularly the 50 word model on longer sentences.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:attention_results"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Residual Network 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "resnet"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
A residual network learns a function for the difference between the inputs and the outputs rather than a function to express the outputs given the inputs. Instead of learning the function 
\begin_inset Formula $F$
\end_inset

, 
\begin_inset Formula \begin{align}
	\mathbf{y} = F(\mathbf{x})
\end{align}
\end_inset

the residual network learns the function 
\begin_inset Formula $H$
\end_inset

 
\begin_inset Formula \begin{align}
	\mathbf{y} = H(\mathbf{x}) - \mathbf{x}
\end{align}
\end_inset

The residual network is motivated by the 
\shape italic
degradation problem
\shape default
, where an increase in the number of layers results in a disimprovement in both training and test error. Theoretically, if the added layers just learn the identity function there should be no degradation. This shows that some systems are more diffcult to optimise than others. The idea of the residual network is that it might be easier to optimise a network that learns residual functions instead of the usual output functions.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/residual_rnn.png
	width 50text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A building block for a residual network. The input for the top layer is projected to the bottom layer, so the block learns a function for the difference between the input and output, i.e. the residual.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:residual_rnn"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In a residual network, the inputs for some given layer are fed back into the network two or three layers down. A block of a residual network can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:residual_rnn"
plural "false"
caps "false"
noprefix "false"

\end_inset

. A residual network was used to perform classification on the ImageNet database, and the inclusaion of the residual innovation on top of all the other tricks resulted in a classification error lower than human error 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "resnet"
literal "true"

\end_inset

. An example 34 layer neural network used on the ImageNet database, with and without residual connections is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:resnet"
plural "false"
caps "false"
noprefix "false"

\end_inset

. 
\begin_inset Float figure
placement p
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/resnet.png
	width 42text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
(left) A network architecture with 34 layers that was applied to the ImageNet database. (right) The same architecture with residual connections, making it a residual network.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:resnet"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Residual LSTM 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "kim"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
A residual LSTM is a standard LSTM with a residual innovation where the output of an LSTM in a higher layer is passed in additively before the output gate is applied. Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cand"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_update"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_forget"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_output"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lstm_cell"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are all the same. A 
\shape italic
projection output
\shape default
 is defined by 
\begin_inset Formula \begin{align}
	m^{\langle t \rangle (l)} = W_m \tanh(c^{\langle t \rangle (l)})
\end{align}
\end_inset

the output from the previous layer's LSTM is added to this projection, then the output gate is applied 
\begin_inset Formula \begin{align}
	a^{\langle t \rangle (l)} = \Gamma_o \odot \left(m^{\langle t \rangle (l)} + W_{a^{\prime}}a^{\langle t \rangle (l-1)} \right) = \Gamma_o \odot \left(m^{\langle t \rangle (l)} + W_{a^{\prime}}x^{\langle t \rangle (l)} \right)
\end{align}
\end_inset

Essentially, the input to the LSTM is carried forward and reinserted just before last step of the LSTM processing. Each LSTM unit is a residual building block, similar but more complicated to that shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:residual_rnn"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
in 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "kim"
literal "true"

\end_inset

, Kim et al used the AMI meeting corpus to evaluate residual LSTMs compared to standard LSTMs and highway LSTMs (LSTMs with a parametrised spacial skip connection). The AMI meeting corpus is a dataset of recordings of people meeting up and speaking English. The RNNs were trained to predict the next word for each speaker speaking at any given time. Both 3-layer and 10-layer networks were trained. The results of the AMI assessment for highway and residual LSTMs are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lstm_AMI"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Both the training and cross-validation error for the 10-layer highway LSTM network are higher than that for the 3-layer. This indicates that the highway LSTM suffers from degredation. For the residual LSTM, although the training error is higher for the 10-layer network, the cross-validation error is lower for the deeper network. So the residual LTSM does not suffer from degredation. The higher training error for the deeper network could be due to better generalisation for data outside of the training set.
\end_layout

\begin_layout Standard

\begin_inset Float figure
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.49
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/highway_lstm_AMI.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:highway_lstm_AMI"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.49
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/residual_lstm_AMI.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:residual_lstm_AMI"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
(a) The results of the AMI assessment on the highway LSTM network. Note that for both training and cross-validation, the cross-entropy error is higher for the deeper network. (b) The results of the AMI assessment on the residual LSTM network. Although the training error is higher for the deeper network, the cross validation error is lower for the deeper network.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:lstm_AMI"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\shape italic
word error rate
\shape default
 (WER) for overlapping and non-overlapping speech was also assessed for standard LSTMs, highway LSTMs, and residual LSTMs. 3-layer, 5-layer, and 10-layer versions of each network were used. The results are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wer"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Both the standard and highway LSTM networks have higher word error rates in general than the residual LSTM, and increasing word error rates for deeper networks. The residual LSTM network shows improving performance for deeper networks for non-overlapping speech.
\end_layout

\begin_layout Standard

\begin_inset Float figure
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/wer.png
	width 75text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Word error rates for overlapped and non-overlapped speech for standard LSTMs, highway LSTMS, and residual LSTMs, of depth 3, 5, or 10.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:wer"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Example: Exponential Smooting RNN - M4 Competition Winner 
\begin_inset CommandInset citation
LatexCommand cite
after ""
key "smyl"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The winner of the M4 competition, a competition for time series forecasting, is a hybrid model combining exponential smoothing and RNNs. The model was implemented using DyNet and 
\family typewriter
C++
\family default
.
\end_layout

\begin_layout Subsubsection
Holt-Winter's Decomposition Preprocessing
\end_layout

\begin_layout Standard
RNNs require a large amount of data in order to be trained. This usually means that they are trained with time series from different contexts. As a result of this, RNN models tend to over-generalise and their forecasts are not time-series specific. One particular problem is that RNN models struggle with seasonality. Smyl et al (2018) deal with this problem by using an Holt-Winter's seasonal model decomposition (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:hw_seasonal"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as a preprocessing step. For each time series in the M4 dataset, the level and seasonality are calculated as 
\begin_inset Formula \begin{align}
	\ell_{t} &= \alpha \frac{y_t}{s_t} + (1 - \alpha)\ell_{t-1} \\
	s_{t+m} &= \gamma\frac{y_t}{l_t} + (1 - \gamma)s_t
\end{align}
\end_inset

then the forecasting equation is 
\begin_inset Formula \begin{align}
	\hat{y}_{t+1, \dots, t+h} = RNN(X_t)\odot \ell_t \odot s_{t+1, \dots, t+h}
\end{align}
\end_inset

where 
\begin_inset Formula $X_t$
\end_inset

 is a vector of a normalised and deseasonlised time series features, usually the length of one season, and includes the origin of the series in a binary vector the length of the number of categories.
\end_layout

\begin_layout Standard
Bear in mind that the parameters for the Holt-Winter's decomposition 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

, are learned for each time series separately. The parameters in the RNN are learned for all of the time series together. Therefore the mode is hierarchical in nature, part of the model deals with the time series individually, and part of the model deals with all of the dataset as one.
\end_layout

\begin_layout Subsubsection
RNN Architectures
\end_layout

\begin_layout Standard
Three different RNN architectures were used to model yearly, quarterly, and monthly time series. A schematic diagram of the RNN for yearly data is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:uber_yearly"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The network consists of two layers of LSTM units using the attention mechanism and dilations, followed by a fully connected (aka dense) layer with a 
\begin_inset Formula $\tanh$
\end_inset

 activation function, followed by a linear adaptor layer. The linear adaptor layer transforms the output of the non-linear layer into the shape of the forecast horizon and produces upper and lower prediction intervals.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement ht
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.32
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/uber_yearly.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:uber_yearly"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.32
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/uber_quarterly.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:uber_quarterly"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.32
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename ../latex/forecasting lit review/figures/uber_monthly.png
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:uber_monthly"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
(a) A schematic diagram of the RNN architecture used by Smyl et al (2018) for forecasting 
\series bold
yearly
\series default
 time series. (b) A schematic diagram of the RNN architecture used by Smyl et al (2018) for forecasting 
\series bold
quarterly
\series default
 time series. (c) A schematic diagram of the RNN architecture used by Smyl et al (2018) for forecasting 
\series bold
monthly
\series default
 time series.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A schematic diagram of the RNN used for quarterly data is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:uber_quarterly"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The network consists of four layers of LSTM units with exponentially increasing dilations. The top two layers of LSTMs make up a `classical' residual block. The output layer is a linear adaptor that converts the size of the LSTM output to the size of the forecast horizon.
\end_layout

\begin_layout Standard
A schematic diagram of the RNN used for the monthly data is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:uber_monthly"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The network consists of four layers of residual LTSM units. Note that these are residual LSTMs, not residual connections around standard LSTMs. The output layer is a linear adaptor similar to that in the RNN for quarterly data.
\end_layout

\begin_layout Subsubsection
Loss Function
\end_layout

\begin_layout Standard
The ES-RNN model uses a 
\shape italic
Pinball loss function
\shape default
.
\end_layout

\begin_layout Standard
A Pinball loss function is a loss function that measures the difference between the actual and forecasted values and introduces some bias to penalise positive errors more than negative errors, or negative errors more than positive errors. A Pinball loss function takes the form of 
\begin_inset Formula \begin{align}
	L_Q(y_t - \hat{y}_t) =  	\begin{cases}
						Q(y_t - \hat{y}_t) &\quad \text{if } y_t \geq \hat{y}_t \\
						(Q-1)(y_t - \hat{y}_t) &\quad \text{if } y_t < \hat{y}_t
					\end{cases}
\end{align}
\end_inset

where 
\begin_inset Formula $0 \leq Q \leq 1$
\end_inset

. If 
\begin_inset Formula $Q = 0.5$
\end_inset

 then 
\begin_inset Formula $L_Q(y_t - \hat{y}_t)$
\end_inset

 is equivalent to the 
\begin_inset Formula $\ell_1$
\end_inset

 loss function. If 
\begin_inset Formula $Q > 0.5$
\end_inset

 then errors where 
\begin_inset Formula $y_t \geq \hat{y}_t$
\end_inset

 will be penalised more heavily than errors where 
\begin_inset Formula $y_t \leq \hat{y}_t$
\end_inset

.
\end_layout

\begin_layout Standard
For the ES-RNN model, 
\begin_inset Formula $Q = 0.48$
\end_inset

. This value was chosen to counteract positive bias that the authors noticed while developing the model.
\end_layout

\begin_layout Standard
For assessing the quality of prediction intervals, the M4 competition used the 
\shape italic
Mean Scaled Interval Score
\shape default
, or MSIS, 
\begin_inset Formula \begin{align}
	MSIS(U_t, L_t, Y_t, h, a) = \frac{1}{h}\frac{\sum_{t=1}^h (U_t - L_t) + \frac{2}{a}(L_t - Y_t)\mathbf{1}\{ Y_t < L_t \} + \frac{2}{a}(Y_t - U_t)\mathbf{1}\{ Y_t > U_t \}}{\frac{1}{n-m}\sum_{t=m+1}^n\modu{Y_t - Y_{t-m}}}
\end{align}
\end_inset

where 
\begin_inset Formula $U_t$
\end_inset

 is the estimate for the upper prediction interval, 
\begin_inset Formula $L_t$
\end_inset

 is the estimate for the lower prediction interval, 
\begin_inset Formula $h$
\end_inset

 is the horizon, 
\begin_inset Formula $a$
\end_inset

 is a significance level ,and 
\begin_inset Formula \begin{align}
	\mathbf{1}\{ \text{\textbf{conditional}} \} = 	\begin{cases}
									1 &\quad \text{ if \textbf{conditional} is true} \\
									0 &\quad \text{ if \textbf{conditional} is false} \\
								\end{cases}
\end{align}
\end_inset

is called the 
\shape italic
indicator function
\shape default
. The ES-RNN model used the numerator of the MSIS function to train the model when forecasting prediction intervals.
\end_layout

\begin_layout Standard

\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "forecasting_lit_review.bbl"
options "default"

\end_inset


\end_layout

\end_body
\end_document
